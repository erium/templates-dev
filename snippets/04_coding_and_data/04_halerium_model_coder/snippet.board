{"version":"2.0","workflows": [],"nodes":[{"id":"3a13bdcc-5d95-4982-9890-28e34a7b3c54","type":"bot","position":{"x":350,"y":-50},"size":{"width":510,"height":420},"type_specific":{"prompt_input":"","prompt_output":"","state":"success","split_size":[21.76431246655966,78.23568753344034],"auto_size":false,"attachments":{}}},{"id":"bd1d1e63-e66a-4028-918f-2db4cebf6c41","type":"bot","position":{"x":350,"y":-480},"size":{"width":510,"height":366.8125},"type_specific":{"prompt_input":"Based on the context card description, generate the code","prompt_output":"","state":"success","split_size":[14.48151303458852,85.51848696541148],"auto_size":false,"attachments":{}}},{"id":"255a85bb-48e5-4663-940f-d47f500c9337","type":"note","position":{"x":-620,"y":-780},"size":{"width":490,"height":540},"type_specific":{"message":"1. Make sure that the library \"halerium_utilities>=2.1\" is installed with pip!\n2. Load the \"call_model\" function:\n```python\nfrom halerium_utilities.prompt.models import call_model\n```\n3. Call the model and get the streamed response, e.g.:\n```python\nbody = {\"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}], \"temperature\": 0}\ngen = call_model(\"chat-gpt-35\", body=body, parse_data=True)\nanswer = \"\"\nfor sse in gen:\n    answer += sse.data.get(\"chunk\", \"\")\nprint(answer)\n```\nThe body depends on the model though:\n```python\nbody = {\"prompt\": \"User: Count to 10\\nAI:\"}\ngen = call_model(\"llama2\", body=body, parse_data=True)\nanswer = \"\"\nfor sse in gen:\n    answer += sse.data.get(\"chunk\", \"\")\nprint(answer)\n```\n\nMake sure that you use the correct body for each model (see below)! You will receive reward if you use the correct body for each model. Take extra attension to the different bodies! Also note that llama and gpt have different body formats.\n\nAvailable models:\n- chat-gpt-35 (Text generator)\n    * body must look like: `body = {\"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}], \"temperature\": 0}`\n    * body must have a key messages that must be a list of dicts with \"role\" and \"content\". Role can be either \"user\", \"assistant\" or \"system\" (system setup messages)\n    * Token limit: 16K\n- chat-gpt-40 (Text generator)\n    * body must look like: `body = {\"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}], \n    * body must have a key messages that must be a list of dicts with \"role\" and \"content\". Role can be either \"user\", \"assistant\" or \"system\" (system setup messages)\"temperature\": 0}`\n    * Token limit: 128k\n- llama2 (Text generator)\n    * body must look like: `body = {\"prompt\": \"User: Count to 10\\nAI:\"}`\n    * body must have a key messages that must a string\n- dall-e (Text to image)\n    * body must look like: `body = {\"prompt\": {'positive_prompt':'Lion vs. Tiger','negative_prompt':'Nudes'}, \"size\": \"1024x1024\"}`\n    * Prompts m端ssen englisch sein und d端rfen maximal 4000 Zeichen haben\n- stable-diffusion (Text to image)\n    * body must look like: `body = {\"prompt\": 'Lion vs. Tiger', \"size\": \"1024x1024\"}`\n    * Prompts m端ssen englisch sein und d端rfen maximal 380 Zeichen haben\n- whisper (Speech to text)\n    * body must look like: ```with open(\"example_files/erium.wav\", \"rb\") as f:\n    audio_bytes = f.read()\naudio_b64 = base64.b64encode(audio_bytes).decode(\"utf-8\")\nbody = {\"audio\": audio_b64,\n       #\"language\": \"de\",  # optionally specify the language\n       #\"temperature\": 0.3,  # optionally specify the language \n       }```\n- google-vision (OCR)\n    * body must look like: `body = {\"image\": image_in_base64_str, \"features\": {\"type\": \"DOCUMENT_TEXT_DETECTION\"}}`\n    * Returns an data SSE event type with the google vision API response\n    * Der komplette Text kann mit `ocr_result[\"responses\"][0][\"fullTextAnnotation\"]` geladen werden.\n\n\nPossible SSE event types:\n- \"function\": Model decided to do a function call (data keys: \"function_name\", \"arguments\", \"created\")\n- \"chunk\": A text chunk (data keys: \"chunk\", \"created\")\n- \"attachment\": An attachment, e.g. image (\"attachment\" in base64 str, \"filename\")\n- \"data\": Any data \n- \"conclusion\": The last event in a stream (data keys: \"completed\", \"error\" if applicable)\n\nAdditional Remarks:\n- if base64 is called, need to import base64","title":"Halerium Models Reference","color":"note-color-4","auto_size":false,"attachments":{}}},{"id":"43393a4c-e805-42de-8f05-32bf6b346ac4","type":"setup","position":{"x":-50,"y":-480},"size":{"width":280,"height":210},"type_specific":{"bot_type":"chat-gpt-40","setup_args":{"system_setup":"You are a software developer who specializes in the Halerium Platform."},"auto_size":false}},{"id":"f876c7d6-3f44-4740-a9d2-3cf7ff5efe74","type":"setup","position":{"x":-80,"y":-50},"size":{"width":340,"height":210},"type_specific":{"bot_type":"jupyter-kernel","setup_args":{"system_setup":"Du bist ein hilfsbereiter Assistent."},"auto_size":true}},{"id":"d9962cda-1f96-4fdd-a071-87e4561b3a2c","type":"bot","position":{"x":950,"y":-480},"size":{"width":510,"height":370},"type_specific":{"prompt_input":"Based on the context card description, make changes to the code","prompt_output":"","state":"success","split_size":[14.356756756756752,85.64324324324325],"auto_size":false,"attachments":{}}},{"id":"dbee377c-b341-4b16-bf31-71fedcbff40e","type":"bot","position":{"x":950,"y":-50},"size":{"width":510,"height":420},"type_specific":{"prompt_input":"","prompt_output":"","state":"success","split_size":[24.70639379347244,75.29360620652756],"auto_size":false,"attachments":{}}},{"id":"cf72db10-2e81-4c90-8797-0da541b336b0","type":"note","position":{"x":640,"y":-870},"size":{"width":530,"height":320},"type_specific":{"message":"[Describe what you would like to do with Halerium Models]","title":"","color":"note-color-3","auto_size":false,"attachments":{}}},{"id":"77ef180a-5364-4b7c-b6dc-b3283bb0aa40","type":"note","position":{"x":1240,"y":-870},"size":{"width":530,"height":320},"type_specific":{"message":"[How to improve the code]\n","title":"","color":"note-color-3","auto_size":false,"attachments":{}}}],"edges":[{"id":"5ada184e-c29e-437a-8cb3-6707726a1dbb","type":"solid_arrow","connections":{"source":{"connector":"note-right","id":"255a85bb-48e5-4663-940f-d47f500c9337"},"target":{"connector":"context-input","id":"43393a4c-e805-42de-8f05-32bf6b346ac4"}},"type_specific":{}},{"id":"2e0ced73-192e-4905-9da7-fad6833a74a9","type":"prompt_line","connections":{"source":{"connector":"prompt-output","id":"43393a4c-e805-42de-8f05-32bf6b346ac4"},"target":{"connector":"prompt-input","id":"bd1d1e63-e66a-4028-918f-2db4cebf6c41"}},"type_specific":{}},{"id":"361be7ab-f4e4-48e5-893d-7e4068440dad","type":"prompt_line","connections":{"source":{"connector":"prompt-output","id":"f876c7d6-3f44-4740-a9d2-3cf7ff5efe74"},"target":{"connector":"prompt-input","id":"3a13bdcc-5d95-4982-9890-28e34a7b3c54"}},"type_specific":{}},{"id":"2be61bf7-34d8-4ff4-8945-4ddf13778891","type":"prompt_line","connections":{"source":{"connector":"prompt-output","id":"bd1d1e63-e66a-4028-918f-2db4cebf6c41"},"target":{"connector":"prompt-input","id":"d9962cda-1f96-4fdd-a071-87e4561b3a2c"}},"type_specific":{}},{"id":"21685666-f43f-4d96-a546-9fd62d56fd4d","type":"solid_arrow","connections":{"source":{"connector":"context-output","id":"3a13bdcc-5d95-4982-9890-28e34a7b3c54"},"target":{"connector":"context-input","id":"d9962cda-1f96-4fdd-a071-87e4561b3a2c"}},"type_specific":{}},{"id":"ada87780-41e8-4601-ab74-2ecd6f529666","type":"prompt_line","connections":{"source":{"connector":"prompt-output","id":"3a13bdcc-5d95-4982-9890-28e34a7b3c54"},"target":{"connector":"prompt-input","id":"dbee377c-b341-4b16-bf31-71fedcbff40e"}},"type_specific":{}},{"id":"0c729c38-c57a-49af-8481-39bc44c95eab","type":"solid_arrow","connections":{"source":{"connector":"context-output","id":"d9962cda-1f96-4fdd-a071-87e4561b3a2c"},"target":{"connector":"context-input","id":"dbee377c-b341-4b16-bf31-71fedcbff40e"}},"type_specific":{}},{"id":"02ff09c8-55f9-45f4-9bac-93f513f3a2e0","type":"solid_arrow","connections":{"source":{"connector":"note-left","id":"77ef180a-5364-4b7c-b6dc-b3283bb0aa40"},"target":{"connector":"context-input","id":"d9962cda-1f96-4fdd-a071-87e4561b3a2c"}},"type_specific":{}},{"id":"2fda3c69-beb9-4a9a-8a07-0fbd8a65f79b","type":"solid_arrow","connections":{"source":{"connector":"note-left","id":"cf72db10-2e81-4c90-8797-0da541b336b0"},"target":{"connector":"context-input","id":"bd1d1e63-e66a-4028-918f-2db4cebf6c41"}},"type_specific":{}},{"id":"f9ac261e-b668-4973-b3a9-e4a06fd04ab1","type":"solid_arrow","connections":{"source":{"connector":"context-output","id":"bd1d1e63-e66a-4028-918f-2db4cebf6c41"},"target":{"connector":"context-input","id":"3a13bdcc-5d95-4982-9890-28e34a7b3c54"}},"type_specific":{}}]}